'use client'

import React, { useState, useEffect, useRef, useCallback } from 'react'
import { Mic, MicOff, Square } from 'lucide-react'
import { Button } from '@/components/ui/button'
import { useAuth } from '@/hooks/useAuth'
import { useRecordingSessions } from '@/hooks/useRecordingSessions'
import { TranscriptEvent } from '@/lib/supabase'
import { toast } from 'sonner'

interface RecorderProps {
  onTranscript: (transcriptEvent: TranscriptEvent) => void
  onRecordingStateChange: (isRecording: boolean) => void
  onSessionCreated?: (sessionId: string) => void
}

export function Recorder({ onTranscript, onRecordingStateChange, onSessionCreated }: RecorderProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [isConnecting, setIsConnecting] = useState(false)
  const [isMuted, setIsMuted] = useState(false)
  const [audioLevel, setAudioLevel] = useState(0)
  const [currentTime, setCurrentTime] = useState('00:00')
  const [isStopping, setIsStopping] = useState(false)

  const { session: authSession } = useAuth()
  const { createSession, finalizeSession } = useRecordingSessions()

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const sessionIdRef = useRef<string>('')
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number>()
  const eventSourceRef = useRef<EventSource | null>(null)
  const startTimeRef = useRef<number>()
  const transcriptBufferRef = useRef<string>('')

  const showError = useCallback((message: string) => {
    try {
      console.error(message)
      toast.error(message)
    } catch {
      // Fallback in case console.error fails
      try {
        console.log('Error:', message)
      } catch {
        // If even console.log fails, ignore
      }
    }
  }, [])

  const updateAudioLevel = useCallback(() => {
    if (!analyserRef.current) return

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)
    const average = Array.from(dataArray).reduce((a, b) => a + b, 0) / dataArray.length
    setAudioLevel(average / 255)

    animationFrameRef.current = requestAnimationFrame(updateAudioLevel)
  }, [])

  const setupAudioVisualization = useCallback((stream: MediaStream) => {
    audioContextRef.current = new (window.AudioContext || (window as { webkitAudioContext?: typeof AudioContext }).webkitAudioContext || AudioContext)()
    analyserRef.current = audioContextRef.current.createAnalyser()
    const audioSource = audioContextRef.current.createMediaStreamSource(stream)
    audioSource.connect(analyserRef.current)
    analyserRef.current.fftSize = 64
    updateAudioLevel()
  }, [updateAudioLevel])

  const updateTimer = useCallback(() => {
    if (startTimeRef.current) {
      const elapsed = Date.now() - startTimeRef.current
      const minutes = Math.floor(elapsed / 60000)
      const seconds = Math.floor((elapsed % 60000) / 1000)
      setCurrentTime(`${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`)
    }
  }, [])

  useEffect(() => {
    let interval: NodeJS.Timeout
    if (isRecording) {
      interval = setInterval(updateTimer, 1000)
    }
    return () => {
      if (interval) clearInterval(interval)
    }
  }, [isRecording, updateTimer])

  // Component unmount cleanup effect
  useEffect(() => {
    return () => {
      // Clean up all resources when component unmounts
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      if (eventSourceRef.current) {
        eventSourceRef.current.close()
        eventSourceRef.current = null
      }
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close().catch(e => console.error("Error closing AudioContext on unmount:", e))
        audioContextRef.current = null
      }
      if (peerConnectionRef.current) {
        peerConnectionRef.current.getSenders().forEach(sender => {
          if (sender.track) {
            sender.track.stop()
          }
        })
        peerConnectionRef.current.close()
        peerConnectionRef.current = null
      }
    }
  }, [])

  // è®¾ç½®å®æ—¶è½¬å½•ç›‘å¬
  const setupTranscriptListener = useCallback((sessionId: string, token: string) => {
    // åœ¨å¼€å‘ç¯å¢ƒä¸‹ç›´æ¥è¿æ¥åˆ°åç«¯ï¼Œç”Ÿäº§ç¯å¢ƒä½¿ç”¨ä»£ç†
    const transcriptUrl = process.env.NODE_ENV === 'development' 
      ? `http://localhost:8000/transcript?webrtc_id=${sessionId}&token=${token}`
      : `/transcript?webrtc_id=${sessionId}&token=${token}`
    console.log('ğŸ§ å¼€å§‹ç›‘å¬å®æ—¶è½¬å½•:', transcriptUrl)
    
    eventSourceRef.current = new EventSource(transcriptUrl)
    
    eventSourceRef.current.addEventListener('output', (event) => {
      try {
        const transcriptData: TranscriptEvent = JSON.parse(event.data)
        console.log('ğŸ“ æ”¶åˆ°è½¬å½•æ•°æ®:', transcriptData)
        
        // æ›´æ–°è½¬å½•ç¼“å†²åŒº
        if (transcriptData.is_final) {
          transcriptBufferRef.current += transcriptData.text + ' '
        }
        
        // é€šçŸ¥ä¸Šçº§ç»„ä»¶ - ä¼ é€’å®Œæ•´çš„è½¬å½•äº‹ä»¶å¯¹è±¡
        try {
          onTranscript(transcriptData)
        } catch (callbackError) {
          console.error('onTranscript callback error:', callbackError)
        }
      } catch (error) {
        console.error('è§£æè½¬å½•æ•°æ®å¤±è´¥:', error)
      }
    })
    
    eventSourceRef.current.addEventListener('error', (error) => {
      console.error('è½¬å½•ç›‘å¬é”™è¯¯:', error)
      showError('è½¬å½•è¿æ¥ä¸­æ–­')
    })
    
    eventSourceRef.current.addEventListener('open', () => {
      console.log('âœ… è½¬å½•è¿æ¥å·²å»ºç«‹')
    })
  }, [onTranscript, showError])

  const stopRecording = useCallback(async () => {
    // é˜²æ­¢é‡å¤è°ƒç”¨
    if (isStopping) {
      console.log('ğŸš« æ­£åœ¨åœæ­¢å½•éŸ³ä¸­ï¼Œå¿½ç•¥é‡å¤è¯·æ±‚')
      return
    }

    console.log('ğŸ›‘ åœæ­¢å½•éŸ³...')
    
    // ç«‹å³è®¾ç½®åœæ­¢çŠ¶æ€ï¼Œé˜²æ­¢é‡å¤ç‚¹å‡»
    setIsStopping(true)
    
    // ç«‹å³åœæ­¢UIçŠ¶æ€æ›´æ–°ï¼ˆè®¡æ—¶å™¨ã€å½•éŸ³çŠ¶æ€ç­‰ï¼‰
    setIsRecording(false)
    setIsConnecting(false)
    setIsMuted(false)
    setAudioLevel(0)
    setCurrentTime('00:00')
    
    // ç«‹å³é€šçŸ¥ä¸Šçº§ç»„ä»¶çŠ¶æ€å˜åŒ–
    try {
      onRecordingStateChange(false)
    } catch (callbackError) {
      console.error('onRecordingStateChange callback error:', callbackError)
    }
    
    // ç„¶ååœ¨åå°è¿›è¡Œæ¸…ç†å·¥ä½œ
    try {
      // åœæ­¢éŸ³é¢‘å¯è§†åŒ–
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      
      // å…³é—­è½¬å½•ç›‘å¬è¿æ¥
      if (eventSourceRef.current) {
        console.log('ğŸ”Œ å…³é—­è½¬å½•ç›‘å¬è¿æ¥...')
        eventSourceRef.current.close()
        eventSourceRef.current = null
      }
      
      // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close().catch(e => console.error("Error closing AudioContext:", e))
        audioContextRef.current = null
      }
      
      // å…³é—­WebRTCè¿æ¥
      if (peerConnectionRef.current) {
        peerConnectionRef.current.getSenders().forEach(sender => {
          if (sender.track) {
            sender.track.stop()
          }
        })
        peerConnectionRef.current.close()
        peerConnectionRef.current = null
      }

      // å®Œæˆä¼šè¯ï¼ˆåœ¨åå°è¿›è¡Œï¼‰
      if (sessionIdRef.current) {
        console.log('ğŸ å®Œæˆä¼šè¯:', sessionIdRef.current)
        try {
          await finalizeSession(sessionIdRef.current)
          console.log('âœ… ä¼šè¯å®ŒæˆæˆåŠŸ')
        } catch (error) {
          console.error('å®Œæˆä¼šè¯å¤±è´¥:', error)
          // å³ä½¿finalizeå¤±è´¥ï¼Œä¹Ÿä¸å½±å“UIçŠ¶æ€
        }
      }
      
      // æ¸…ç†å¼•ç”¨
      startTimeRef.current = undefined
      sessionIdRef.current = ''
      transcriptBufferRef.current = ''
      
    } catch (error) {
      console.error('åœæ­¢å½•éŸ³æ—¶å‘ç”Ÿé”™è¯¯:', error)
    } finally {
      // é‡ç½®åœæ­¢çŠ¶æ€
      setIsStopping(false)
      console.log('ğŸ å½•éŸ³åœæ­¢å®Œæˆ')
    }
  }, [isStopping, onRecordingStateChange, finalizeSession])

  const startRecording = useCallback(async () => {
    // Prevent multiple concurrent recording attempts
    if (isConnecting || isRecording || isStopping) {
      console.log('ğŸš« å½•éŸ³å·²åœ¨è¿›è¡Œä¸­æˆ–æ­£åœ¨åœæ­¢ï¼Œå¿½ç•¥é‡å¤è¯·æ±‚')
      return
    }

    if (!authSession?.access_token) {
      showError('ç”¨æˆ·æœªç™»å½•')
      return
    }

    // Clean up any existing connections before starting new one
    if (peerConnectionRef.current) {
      console.log('ğŸ§¹ æ¸…ç†ç°æœ‰è¿æ¥...')
      await stopRecording()
      // Wait a bit for cleanup to complete
      await new Promise(resolve => setTimeout(resolve, 500))
    }

    // é‡ç½®æ‰€æœ‰çŠ¶æ€
    setIsStopping(false)
    setIsConnecting(true)
    console.log('ğŸ™ï¸ å¼€å§‹å½•éŸ³æµç¨‹...')
    
    try {
      // Step 0: æ£€æŸ¥å½•éŸ³è®¾å¤‡æ˜¯å¦å­˜åœ¨
      console.log('0ï¸âƒ£ æ£€æŸ¥å½•éŸ³è®¾å¤‡...')
      if (navigator.mediaDevices && navigator.mediaDevices.enumerateDevices) {
        try {
          const devices = await navigator.mediaDevices.enumerateDevices()
          const audioInputDevices = devices.filter(device => device.kind === 'audioinput')
          if (audioInputDevices.length === 0) {
            throw new Error('æœªæ£€æµ‹åˆ°å½•éŸ³è®¾å¤‡ï¼Œè¯·è¿æ¥éº¦å…‹é£åé‡è¯•')
          }
          console.log('âœ… æ£€æµ‹åˆ°å½•éŸ³è®¾å¤‡:', audioInputDevices.length, 'ä¸ª')
        } catch (enumError) {
          console.error('è®¾å¤‡æ£€æŸ¥å¤±è´¥:', enumError)
          throw new Error('æ— æ³•æ£€æµ‹å½•éŸ³è®¾å¤‡ï¼Œè¯·ç¡®è®¤éº¦å…‹é£å·²è¿æ¥')
        }
      } else {
        throw new Error('æµè§ˆå™¨ä¸æ”¯æŒå½•éŸ³åŠŸèƒ½')
      }

      // Step 1: åˆ›å»ºä¸šåŠ¡ä¼šè¯
      console.log('1ï¸âƒ£ åˆ›å»ºä¸šåŠ¡ä¼šè¯...')
      const sessionResult = await createSession('æ–°çš„å½•éŸ³ä¼šè¯')
      
      if (!sessionResult) {
        throw new Error('åˆ›å»ºä¼šè¯å¤±è´¥')
      }
      
      sessionIdRef.current = sessionResult.session_id
      console.log('âœ… ä¼šè¯åˆ›å»ºæˆåŠŸ:', sessionIdRef.current)
      
      // é€šçŸ¥ä¸Šçº§ç»„ä»¶ä¼šè¯å·²åˆ›å»º
      try {
        onSessionCreated?.(sessionResult.session_id)
      } catch (callbackError) {
        console.error('onSessionCreated callback error:', callbackError)
        // Continue execution even if callback fails
      }

      // Step 2: è·å–éŸ³é¢‘æµå¹¶å»ºç«‹WebRTCè¿æ¥
      console.log('2ï¸âƒ£ è·å–éŸ³é¢‘æµ...')
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      setupAudioVisualization(stream)

      // ä½¿ç”¨ç©ºé…ç½®ï¼Œè®©æµè§ˆå™¨ä½¿ç”¨é»˜è®¤STUNæœåŠ¡å™¨
      const config = undefined
      peerConnectionRef.current = new RTCPeerConnection(config)

      stream.getTracks().forEach(track => {
        peerConnectionRef.current!.addTrack(track, stream)
      })

      peerConnectionRef.current.addEventListener('connectionstatechange', () => {
        console.log('WebRTCè¿æ¥çŠ¶æ€:', peerConnectionRef.current?.connectionState)
        if (peerConnectionRef.current?.connectionState === 'connected') {
          setIsConnecting(false)
          setIsRecording(true)
          startTimeRef.current = Date.now()
          try {
            onRecordingStateChange(true)
          } catch (callbackError) {
            console.error('onRecordingStateChange callback error:', callbackError)
          }
          console.log('âœ… WebRTCè¿æ¥å·²å»ºç«‹')
        }
      })

      // ICEå€™é€‰å¤„ç†
      peerConnectionRef.current.onicecandidate = ({ candidate }) => {
        if (candidate) {
          // console.debug("å‘é€ICEå€™é€‰:", candidate)
          fetch('/webrtc/offer', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              candidate: candidate.toJSON(),
              webrtc_id: sessionIdRef.current, // ä½¿ç”¨session_idä½œä¸ºwebrtc_id
              type: "ice-candidate",
            })
          }).catch(e => console.error("å‘é€ICEå€™é€‰å¤±è´¥:", e))
        }
      }

      // åˆ›å»ºæ•°æ®é€šé“
      const dataChannel = peerConnectionRef.current.createDataChannel('text')
      dataChannel.onmessage = async (event) => {
        console.log('æ”¶åˆ°æ•°æ®é€šé“æ¶ˆæ¯:', event.data)
        
        try {
          const eventJson = JSON.parse(event.data)
          if (eventJson.type === "error") {
            showError(eventJson.message)
          } else if (eventJson.type === "send_input") {
            // å‘é€è¾“å…¥ä¿¡å·åˆ°åç«¯
            const response = await fetch('/send_input', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                webrtc_id: sessionIdRef.current,
                transcript: ""
              })
            })
            if (!response.ok) {
              console.error('å‘é€è¾“å…¥ä¿¡å·å¤±è´¥')
            }
          }
        } catch (e) {
          console.error('è§£ææ•°æ®é€šé“æ¶ˆæ¯å¤±è´¥:', e)
        }
      }

      // Step 3: å»ºç«‹WebRTCè¿æ¥
      console.log('3ï¸âƒ£ å»ºç«‹WebRTCè¿æ¥...')
      const offer = await peerConnectionRef.current.createOffer()
      await peerConnectionRef.current.setLocalDescription(offer)

      // ä½¿ç”¨session_idä½œä¸ºwebrtc_idå‘é€offer
      const response = await fetch('/webrtc/offer', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sdp: peerConnectionRef.current.localDescription!.sdp,
          type: peerConnectionRef.current.localDescription!.type,
          webrtc_id: sessionIdRef.current // å…³é”®ï¼šä½¿ç”¨session_id
        })
      })

      if (!response.ok) {
        throw new Error(`WebRTCè¿æ¥å¤±è´¥: ${response.status}`)
      }

      const serverResponse = await response.json()

      if (serverResponse.status === 'failed') {
        showError(serverResponse.meta?.error === 'concurrency_limit_reached'
          ? `è¿æ¥æ•°è¿‡å¤šï¼Œæœ€å¤§é™åˆ¶ä¸º ${serverResponse.meta?.limit}`
          : (serverResponse.meta?.error || 'è¿æ¥å¤±è´¥'))
        await stopRecording()
        return
      }

      // è®¾ç½®è¿œç¨‹æè¿°
      if (serverResponse.sdp) {
        await peerConnectionRef.current.setRemoteDescription({
          type: 'answer',
          sdp: serverResponse.sdp
        })
        console.log('âœ… WebRTC Answerå·²è®¾ç½®')
      }

      // Step 4: è®¾ç½®å®æ—¶è½¬å½•ç›‘å¬
      console.log('4ï¸âƒ£ è®¾ç½®å®æ—¶è½¬å½•ç›‘å¬...')
      setupTranscriptListener(sessionIdRef.current, authSession.access_token)

      console.log('ğŸ‰ å½•éŸ³æµç¨‹å¯åŠ¨å®Œæˆ')
      
    } catch (error) {
      console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', error)
      showError(`å¯åŠ¨å½•éŸ³å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`)
      
      // Reset state on error - but don't call stopRecording if recording never started
      setIsConnecting(false)
      setIsRecording(false)
      setIsStopping(false)
      
      // Only call stopRecording if we actually started recording (have active connections)
      if (peerConnectionRef.current || eventSourceRef.current || audioContextRef.current) {
        await stopRecording()
      }
    }
  }, [isConnecting, isRecording, isStopping, authSession, createSession, setupTranscriptListener, setupAudioVisualization, showError, stopRecording, onRecordingStateChange, onSessionCreated])

  const toggleMute = useCallback(() => {
    if (peerConnectionRef.current) {
      const audioTracks = peerConnectionRef.current.getSenders()
        .map(sender => sender.track)
        .filter(track => track && track.kind === 'audio') as MediaStreamTrack[]
      
      audioTracks.forEach(track => {
        track.enabled = isMuted
      })
    }
    setIsMuted(!isMuted)
  }, [isMuted])

  return (
    <div className="flex flex-col items-center space-y-4">
      {/* éŸ³é¢‘å¯è§†åŒ– */}
      <div className="w-32 h-32 rounded-full bg-gradient-to-r from-blue-400 to-purple-500 flex items-center justify-center relative overflow-hidden">
        <div 
          className="absolute inset-0 bg-white opacity-30 rounded-full transition-transform duration-100"
          style={{ 
            transform: `scale(${1 + audioLevel * 0.5})`,
            filter: `blur(${audioLevel * 2}px)`
          }}
        />
        {isRecording ? (
          <Mic className="w-12 h-12 text-white z-10" />
        ) : (
          <MicOff className="w-12 h-12 text-white z-10" />
        )}
      </div>

      {/* å½•éŸ³æ—¶é•¿ */}
      <div className="text-2xl font-mono text-gray-700">
        {currentTime}
      </div>

      {/* æ§åˆ¶æŒ‰é’® */}
      <div className="flex space-x-4">
        {!isRecording && !isStopping ? (
          <Button
            onClick={startRecording}
            disabled={isConnecting || isRecording || isStopping}
            className="px-6 py-3 bg-red-500 hover:bg-red-600 text-white rounded-full disabled:opacity-50 disabled:cursor-not-allowed"
          >
            {isConnecting ? 'è¿æ¥ä¸­...' : 'å¼€å§‹å½•éŸ³'}
          </Button>
        ) : (
          <>
            <Button
              onClick={toggleMute}
              variant="outline"
              className="px-4 py-2 rounded-full"
              disabled={!isRecording || isStopping}
            >
              {isMuted ? <MicOff className="w-5 h-5" /> : <Mic className="w-5 h-5" />}
            </Button>
            <Button
              onClick={stopRecording}
              className="px-6 py-3 bg-gray-500 hover:bg-gray-600 text-white rounded-full disabled:opacity-50 disabled:cursor-not-allowed"
              disabled={isStopping}
            >
              <Square className="w-5 h-5 mr-2" />
              {isStopping ? 'æ­£åœ¨åœæ­¢...' : 'åœæ­¢å½•éŸ³'}
            </Button>
          </>
        )}
      </div>

      {/* çŠ¶æ€æŒ‡ç¤º */}
      {isConnecting && (
        <div className="text-sm text-gray-500">
          æ­£åœ¨å»ºç«‹è¿æ¥...
        </div>
      )}
      {isRecording && !isStopping && (
        <div className="text-sm text-green-600">
          ğŸ”´ å½•éŸ³ä¸­...
        </div>
      )}
      {isStopping && (
        <div className="text-sm text-blue-600">
          ğŸ”„ æ­£åœ¨åœæ­¢å½•éŸ³ï¼Œå¤„ç†æ•°æ®ä¸­...
        </div>
      )}
    </div>
  )
} 