'use client'

import React, { useState, useEffect, useRef, useCallback, useMemo } from 'react'
import { Mic, MicOff, Square } from 'lucide-react'
import { Button } from '@/components/ui/button'
import { useAuth } from '@/hooks/useAuth'
import useDirectLiveKit from '@/hooks/useDirectLiveKit'
import { TranscriptEvent } from '@/lib/supabase'
import { toast } from 'sonner'
// ç§»é™¤æœ¬åœ°tokenç”Ÿæˆï¼Œä½¿ç”¨åç«¯API
import { 
  Room, 
  RoomEvent, 
  Track,
  LocalAudioTrack,
  RemoteAudioTrack
} from 'livekit-client'
import { 
  RoomContext,
  useRoomContext,
  RoomAudioRenderer,
  useTracks,
  StartAudio
} from '@livekit/components-react'

interface DirectLiveKitRecorderProps {
  onTranscript: (transcriptEvent: TranscriptEvent) => void
  onRecordingStateChange: (isRecording: boolean) => void
  onSessionCreated?: (sessionId: string) => void
}

// å†…éƒ¨å½•éŸ³ç»„ä»¶å®ç°
function DirectLiveKitRecorderInner({ onTranscript, onRecordingStateChange, onSessionCreated }: DirectLiveKitRecorderProps) {
  const [sessionStarted, setSessionStarted] = useState(false)
  const [isConnecting, setIsConnecting] = useState(false)
  const [isMuted, setIsMuted] = useState(false)
  const [audioLevel, setAudioLevel] = useState(0)
  const [currentTime, setCurrentTime] = useState('00:00')
  const [roomName, setRoomName] = useState('')

  const { session: authSession } = useAuth()
  const room = useRoomContext()
  
  // ä½¿ç”¨ç›´æ¥è¿æ¥LiveKitçš„hook
  const { createRoomConfig } = useDirectLiveKit({
    agentName: 'intrascribe-agent-session',
    title: 'æ–°å½•éŸ³ä¼šè¯',
    language: 'zh-CN'
  })

  const startTimeRef = useRef<number>()
  const animationFrameRef = useRef<number>()
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)

  // è·å–éŸ³é¢‘è½¨é“è¿›è¡Œå¯è§†åŒ–
  const tracks = useTracks([Track.Source.Microphone], { onlySubscribed: false })

  const showError = useCallback((message: string) => {
    console.error(message)
    toast.error(message)
  }, [])

  const updateAudioLevel = useCallback(() => {
    if (!analyserRef.current) return

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)
    const average = Array.from(dataArray).reduce((a, b) => a + b, 0) / dataArray.length
    setAudioLevel(average / 255)

    animationFrameRef.current = requestAnimationFrame(updateAudioLevel)
  }, [])

  const setupAudioVisualization = useCallback((audioTrack: LocalAudioTrack | RemoteAudioTrack) => {
    try {
      const mediaStreamTrack = audioTrack.mediaStreamTrack
      const stream = new MediaStream([mediaStreamTrack])
      
      audioContextRef.current = new (window.AudioContext || (window as unknown as { webkitAudioContext: typeof AudioContext }).webkitAudioContext)()
      analyserRef.current = audioContextRef.current.createAnalyser()
      const audioSource = audioContextRef.current.createMediaStreamSource(stream)
      audioSource.connect(analyserRef.current)
      analyserRef.current.fftSize = 64
      updateAudioLevel()
    } catch (error) {
      console.error('è®¾ç½®éŸ³é¢‘å¯è§†åŒ–å¤±è´¥:', error)
    }
  }, [updateAudioLevel])

  const updateTimer = useCallback(() => {
    if (startTimeRef.current) {
      const elapsed = Date.now() - startTimeRef.current
      const minutes = Math.floor(elapsed / 60000)
      const seconds = Math.floor((elapsed % 60000) / 1000)
      setCurrentTime(`${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`)
    }
  }, [])

  // è®¡æ—¶å™¨æ•ˆæœ
  useEffect(() => {
    let interval: NodeJS.Timeout
    if (sessionStarted) {
      interval = setInterval(updateTimer, 1000)
    }
    return () => {
      if (interval) clearInterval(interval)
    }
  }, [sessionStarted, updateTimer])

  // æ¸…ç†èµ„æº
  useEffect(() => {
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close().catch(e => console.error("Error closing AudioContext:", e))
      }
    }
  }, [])

  // æˆ¿é—´äº‹ä»¶ç›‘å¬
  useEffect(() => {
    if (!room) return

    const handleDataReceived = (payload: Uint8Array) => {
      try {
        const textData = new TextDecoder().decode(payload)
        const transcriptData: TranscriptEvent = JSON.parse(textData)
        console.log('ğŸ“ æ”¶åˆ°è½¬å½•æ•°æ®:', transcriptData)
        onTranscript(transcriptData)
      } catch (error) {
        console.error('è§£æè½¬å½•æ•°æ®å¤±è´¥:', error)
      }
    }

    const handleConnected = () => {
      console.log('âœ… LiveKitæˆ¿é—´å·²è¿æ¥')
      setIsConnecting(false)
      setSessionStarted(true)
      startTimeRef.current = Date.now()
      onRecordingStateChange(true)
      
      // ä½¿ç”¨æˆ¿é—´åç§°ä½œä¸ºä¼šè¯ID
      if (room.name) {
        setRoomName(room.name)
        onSessionCreated?.(room.name)
      }
    }

    const handleDisconnected = () => {
      console.log('ğŸ”Œ LiveKitæˆ¿é—´å·²æ–­å¼€')
      setSessionStarted(false)
      setIsConnecting(false)
      onRecordingStateChange(false)
      setCurrentTime('00:00')
      startTimeRef.current = undefined
    }

    const handleMediaDevicesError = (error: Error) => {
      showError(`åª’ä½“è®¾å¤‡é”™è¯¯: ${error.name}: ${error.message}`)
    }

    // æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
    room.on(RoomEvent.DataReceived, handleDataReceived)
    room.on(RoomEvent.Connected, handleConnected)
    room.on(RoomEvent.Disconnected, handleDisconnected)
    room.on(RoomEvent.MediaDevicesError, handleMediaDevicesError)

    return () => {
      room.off(RoomEvent.DataReceived, handleDataReceived)
      room.off(RoomEvent.Connected, handleConnected)
      room.off(RoomEvent.Disconnected, handleDisconnected)
      room.off(RoomEvent.MediaDevicesError, handleMediaDevicesError)
    }
  }, [room, onTranscript, onRecordingStateChange, onSessionCreated, showError])

  // ç›‘å¬éŸ³é¢‘è½¨é“å˜åŒ–ï¼Œè®¾ç½®å¯è§†åŒ–
  useEffect(() => {
    if (tracks.length > 0) {
      const trackRef = tracks[0]
      const audioTrack = trackRef.publication?.track
      if (audioTrack instanceof LocalAudioTrack || audioTrack instanceof RemoteAudioTrack) {
        setupAudioVisualization(audioTrack)
      }
    }
  }, [tracks, setupAudioVisualization])

  const startRecording = useCallback(async () => {
    if (isConnecting || sessionStarted) {
      console.log('ğŸš« å½•éŸ³å·²åœ¨è¿›è¡Œä¸­ï¼Œå¿½ç•¥é‡å¤è¯·æ±‚')
      return
    }

    if (!authSession?.access_token) {
      showError('ç”¨æˆ·æœªç™»å½•')
      return
    }

    setIsConnecting(true)
    console.log('ğŸ™ï¸ å¼€å§‹LiveKitå½•éŸ³ä¼šè¯...')

    try {
      // 1. è·å–è¿æ¥é…ç½®å’Œtokenï¼ˆåŒæ—¶åˆ›å»ºä¼šè¯è®°å½•ï¼‰
      console.log('1ï¸âƒ£ è·å–LiveKitè¿æ¥é…ç½®å¹¶åˆ›å»ºä¼šè¯è®°å½•...')
      const connectionConfig = await createRoomConfig()

      // 2. é€šçŸ¥çˆ¶ç»„ä»¶ä¼šè¯å·²åˆ›å»º
      if (connectionConfig.sessionId && onSessionCreated) {
        console.log('ğŸ“ é€šçŸ¥çˆ¶ç»„ä»¶ä¼šè¯å·²åˆ›å»º:', connectionConfig.sessionId)
        onSessionCreated(connectionConfig.sessionId)
      }

      // 3. è¿æ¥åˆ°LiveKitæˆ¿é—´
      console.log('2ï¸âƒ£ è¿æ¥LiveKitæˆ¿é—´:', connectionConfig.roomName)
      await room.connect(connectionConfig.serverUrl, connectionConfig.token)

      // 4. è¿æ¥æˆåŠŸåå†å¯ç”¨éº¦å…‹é£
      console.log('3ï¸âƒ£ å¯ç”¨éº¦å…‹é£...')
      await room.localParticipant.setMicrophoneEnabled(true)

      console.log('ğŸ‰ LiveKitå½•éŸ³æµç¨‹å¯åŠ¨å®Œæˆ')

    } catch (error) {
      console.error('å¯åŠ¨LiveKitå½•éŸ³å¤±è´¥:', error)
      showError(`å¯åŠ¨å½•éŸ³å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`)
      
      setIsConnecting(false)
      setSessionStarted(false)
    }
  }, [isConnecting, sessionStarted, authSession, createRoomConfig, room, showError, onSessionCreated])

  const stopRecording = useCallback(() => {
    if (!sessionStarted) {
      return
    }

    console.log('ğŸ›‘ åœæ­¢å½•éŸ³...')
    
    // æ¸…ç†éŸ³é¢‘å¯è§†åŒ–
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
    }

    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close().catch(e => console.error("Error closing AudioContext:", e))
      audioContextRef.current = null
    }

    // æ–­å¼€è¿æ¥
    room.disconnect()
    
    setSessionStarted(false)
    setIsConnecting(false)
    setIsMuted(false)
    setAudioLevel(0)
  }, [sessionStarted, room])

  const toggleMute = useCallback(async () => {
    try {
      await room.localParticipant.setMicrophoneEnabled(isMuted)
      setIsMuted(!isMuted)
    } catch (error) {
      console.error('åˆ‡æ¢é™éŸ³çŠ¶æ€å¤±è´¥:', error)
    }
  }, [room, isMuted])

  return (
    <div className="flex flex-col items-center space-y-6">
      {/* éŸ³é¢‘å¯è§†åŒ– */}
      <div 
        className="w-32 h-32 rounded-full bg-gradient-to-r from-blue-400 to-purple-500 flex items-center justify-center relative overflow-hidden transition-all duration-100"
        style={{
          transform: sessionStarted ? `scale(${1 + audioLevel * 0.3})` : 'scale(1)',
          boxShadow: sessionStarted ? `0 0 ${20 + audioLevel * 40}px rgba(79, 70, 229, 0.6)` : '0 0 0px rgba(79, 70, 229, 0)'
        }}
      >
        <div 
          className="absolute inset-0 bg-white opacity-20 rounded-full transition-transform duration-100"
          style={{ 
            transform: `scale(${1 + audioLevel * 0.5})`,
            filter: `blur(${audioLevel * 2}px)`
          }}
        />
        {sessionStarted && !isMuted ? (
          <Mic className="w-12 h-12 text-white z-10" />
        ) : (
          <MicOff className="w-12 h-12 text-white z-10" />
        )}
      </div>

      {/* å½•éŸ³æ—¶é•¿ */}
      <div className="text-2xl font-mono text-gray-700">
        {currentTime}
      </div>

      {/* æˆ¿é—´ä¿¡æ¯ */}
      {roomName && (
        <div className="text-sm text-gray-500">
          æˆ¿é—´: {roomName}
        </div>
      )}

      {/* æ§åˆ¶æŒ‰é’® */}
      <div className="flex space-x-4">
        {!sessionStarted ? (
          <Button
            onClick={startRecording}
            disabled={isConnecting}
            className="px-8 py-3 bg-red-500 hover:bg-red-600 text-white rounded-full disabled:opacity-50 disabled:cursor-not-allowed"
          >
            {isConnecting ? 'è¿æ¥ä¸­...' : 'å¼€å§‹å½•éŸ³'}
          </Button>
        ) : (
          <>
            <Button
              onClick={toggleMute}
              variant="outline"
              className="px-4 py-2 rounded-full"
              disabled={!sessionStarted || isConnecting}
            >
              {isMuted ? <MicOff className="w-5 h-5" /> : <Mic className="w-5 h-5" />}
            </Button>
            <Button
              onClick={stopRecording}
              className="px-6 py-3 bg-gray-500 hover:bg-gray-600 text-white rounded-full"
            >
              <Square className="w-5 h-5 mr-2" />
              åœæ­¢å½•éŸ³
            </Button>
          </>
        )}
      </div>

      {/* è¿æ¥çŠ¶æ€æ˜¾ç¤º */}
      {isConnecting && (
        <div className="text-sm text-gray-500 animate-pulse">
          æ­£åœ¨è¿æ¥LiveKitæœåŠ¡å™¨...
        </div>
      )}
    </div>
  )
}

// ä¸»è¦çš„å½•éŸ³ç»„ä»¶ï¼ŒåŒ…å«RoomContext
export function DirectLiveKitRecorder({ onTranscript, onRecordingStateChange, onSessionCreated }: DirectLiveKitRecorderProps) {
  const room = useMemo(() => new Room({
    // è‡ªé€‚åº”æµè´¨é‡ä¼˜åŒ–
    adaptiveStream: true,
    // å¯ç”¨åŠ¨æ€è´¨é‡ä¼˜åŒ–
    dynacast: true,
  }), [])

  // æ¸…ç†æˆ¿é—´èµ„æº
  useEffect(() => {
    return () => {
      room.disconnect()
    }
  }, [room])

  return (
    <RoomContext.Provider value={room}>
      <div data-lk-theme="default">
        <DirectLiveKitRecorderInner 
          onTranscript={onTranscript}
          onRecordingStateChange={onRecordingStateChange}
          onSessionCreated={onSessionCreated}
        />
        {/* LiveKitéŸ³é¢‘æ¸²æŸ“å™¨ */}
        <RoomAudioRenderer />
        {/* è‡ªåŠ¨å¯åŠ¨éŸ³é¢‘ */}
        <StartAudio label="å¯åŠ¨éŸ³é¢‘" />
      </div>
    </RoomContext.Provider>
  )
}
