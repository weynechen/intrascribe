"""
é€šç”¨éŸ³é¢‘å¤„ç†æœåŠ¡
æŠ½è±¡è¯´è¯äººåˆ†ç¦»ã€éŸ³é¢‘åˆ†å‰²å’Œè½¬å½•çš„é€šç”¨é€»è¾‘
"""
import logging
import tempfile
import os
from typing import Dict, Any, List, Tuple, Optional
import numpy as np
import re


from .speaker_diarization import speaker_diarization_service, SpeakerSegment
from .audio_converter import audio_converter
from .clients import stt_client
from .models import TranscriptionSegment

logger = logging.getLogger(__name__)


class AudioProcessingService:
    """é€šç”¨éŸ³é¢‘å¤„ç†æœåŠ¡"""
    
    def __init__(self):
        self.speaker_service = speaker_diarization_service
    
    async def process_audio_with_speaker_diarization(
        self, 
        audio_data: bytes, 
        file_format: str = "mp3",
        original_filename: str = "audio"
    ) -> Dict[str, Any]:
        """
        å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œæ‰§è¡Œè¯´è¯äººåˆ†ç¦»å’Œè½¬å½•
        
        Args:
            audio_data: éŸ³é¢‘æ–‡ä»¶æ•°æ®ï¼ˆå­—èŠ‚ï¼‰
            file_format: éŸ³é¢‘æ ¼å¼
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            Dict: å¤„ç†ç»“æœåŒ…å«è½¬å½•ç‰‡æ®µã€è¯´è¯äººæ•°ç­‰
        """
        try:
            logger.info(f"ğŸµ å¼€å§‹å¤„ç†éŸ³é¢‘: {original_filename}, æ ¼å¼: {file_format}")
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œå¤„ç†
            with tempfile.NamedTemporaryFile(suffix=f".{file_format}", delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # è½¬æ¢éŸ³é¢‘æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                processed_audio_path, was_converted, converted_file_path = await self._prepare_audio_for_processing(
                    temp_audio_path, file_format
                )
                
                # æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
                logger.info("ğŸ¤ æ‰§è¡Œè¯´è¯äººåˆ†ç¦»...")
                speaker_segments = await self.speaker_service.diarize_audio_file(processed_audio_path)
                
                if not speaker_segments:
                    logger.warning("è¯´è¯äººåˆ†ç¦»æ— ç»“æœï¼Œä½¿ç”¨å•è¯´è¯äººæ¨¡å¼")
                    audio_duration = await self._get_audio_duration(processed_audio_path)
                    speaker_segments = [SpeakerSegment(
                        start_time=0.0,
                        end_time=audio_duration,
                        speaker_label="speaker_0",
                        duration=audio_duration
                    )]
                
                # åˆå¹¶ç›¸é‚»çš„çŸ­ç‰‡æ®µ
                logger.info("ğŸ”— åˆå¹¶ç›¸é‚»çš„çŸ­ç‰‡æ®µ...")
                speaker_segments = self._merge_adjacent_short_segments(speaker_segments)
                
                # åˆ†å‰²éŸ³é¢‘å¹¶è½¬å½•
                logger.info("âœ‚ï¸ åˆ†å‰²éŸ³é¢‘å¹¶è½¬å½•...")
                transcription_segments, transcription_text = await self._segment_and_transcribe(
                    processed_audio_path, speaker_segments
                )
                
                # ç»Ÿè®¡ç»“æœ
                unique_speakers = len(set(seg.speaker for seg in transcription_segments))
                total_duration = sum(seg.end_time - seg.start_time for seg in transcription_segments)
                
                result = {
                    "success": True,
                    "transcription_segments": transcription_segments,
                    "transcription_text": transcription_text,
                    "speaker_count": unique_speakers,
                    "total_segments": len(transcription_segments),
                    "total_duration": total_duration,
                    "segments_data": [
                        {
                            "index": seg.index,
                            "speaker": seg.speaker,
                            "start_time": seg.start_time,
                            "end_time": seg.end_time,
                            "text": seg.text,
                            "confidence_score": seg.confidence_score,
                            "is_final": seg.is_final
                        }
                        for seg in transcription_segments
                    ]
                }
                
                logger.info(f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆ: ç‰‡æ®µæ•°={len(transcription_segments)}, è¯´è¯äººæ•°={unique_speakers}")
                return result
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                await self._cleanup_temp_files(temp_audio_path, was_converted, converted_file_path)
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "transcription_segments": [],
                "transcription_text": "",
                "speaker_count": 0,
                "total_segments": 0,
                "total_duration": 0.0,
                "segments_data": []
            }
    
    async def _prepare_audio_for_processing(self, audio_path: str, file_format: str) -> Tuple[str, bool, Optional[str]]:
        """å‡†å¤‡éŸ³é¢‘æ–‡ä»¶ç”¨äºå¤„ç†ï¼ˆè½¬æ¢æ ¼å¼å¦‚æœéœ€è¦ï¼‰"""
        processed_audio_path = audio_path
        was_converted = False
        converted_file_path = None
        
        if file_format.lower() in ['mp3', 'mpeg']:
            logger.info("ğŸ”„ è½¬æ¢MP3ä¸ºWAVè¿›è¡Œè¯´è¯äººåˆ†ç¦»...")
            processed_audio_path, was_converted = await audio_converter.convert_to_wav_if_needed(
                audio_path, file_format
            )
            if was_converted:
                converted_file_path = processed_audio_path
        
        return processed_audio_path, was_converted, converted_file_path
    
    async def _segment_and_transcribe(self, audio_path: str, speaker_segments: List[SpeakerSegment]) -> Tuple[List[TranscriptionSegment], str]:
        """åˆ†å‰²éŸ³é¢‘å¹¶è½¬å½•"""
        segment_audios = await self.speaker_service.split_audio_by_segments(audio_path, speaker_segments)
        
        transcription_segments = []
        all_transcription_text = []
        
        for i, (speaker_segment, audio_bytes) in enumerate(segment_audios):
            try:
                # è½¬å½•éŸ³é¢‘ç‰‡æ®µ
                audio_array, sample_rate = self._audio_bytes_to_numpy(audio_bytes)
                transcribed_text = stt_client.transcribe((sample_rate, audio_array))
                
                # æ¸…ç†è½¬å½•æ–‡æœ¬
                cleaned_text = re.sub(r'<\|[^|]*\|>', '', transcribed_text).strip()
                
                if cleaned_text:
                    segment = TranscriptionSegment(
                        index=i + 1,
                        speaker=speaker_segment.speaker_label,
                        start_time=speaker_segment.start_time,
                        end_time=speaker_segment.end_time,
                        text=cleaned_text,
                        confidence_score=None,
                        is_final=True
                    )
                    transcription_segments.append(segment)
                    all_transcription_text.append(cleaned_text)
                    
                    logger.info(f"âœ… è½¬å½•ç‰‡æ®µ {i+1}: {speaker_segment.speaker_label} "
                              f"[{speaker_segment.start_time:.1f}s-{speaker_segment.end_time:.1f}s] "
                              f"æ–‡æœ¬é•¿åº¦: {len(cleaned_text)}")
                else:
                    logger.warning(f"âš ï¸ ç‰‡æ®µ {i+1} è½¬å½•ç»“æœä¸ºç©º")
                    
            except Exception as e:
                logger.error(f"âŒ è½¬å½•ç‰‡æ®µ {i+1} å¤±è´¥: {e}")
                # åˆ›å»ºé”™è¯¯ç‰‡æ®µ
                error_segment = TranscriptionSegment(
                    index=i + 1,
                    speaker=speaker_segment.speaker_label,
                    start_time=speaker_segment.start_time,
                    end_time=speaker_segment.end_time,
                    text=f"è½¬å½•å¤±è´¥: {str(e)}",
                    confidence_score=None,
                    is_final=True
                )
                transcription_segments.append(error_segment)
        
        full_transcription_text = " ".join(all_transcription_text)
        return transcription_segments, full_transcription_text
    
    def _audio_bytes_to_numpy(self, audio_bytes: bytes) -> Tuple[np.ndarray, int]:
        """å°†éŸ³é¢‘å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œè½¬æ¢
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                temp_file.write(audio_bytes)
                temp_file_path = temp_file.name
            
            try:
                # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘
                import librosa
                audio_array, sample_rate = librosa.load(temp_file_path, sr=None)
                
                # è½¬æ¢ä¸ºSTTæœŸæœ›çš„æ ¼å¼ï¼ˆ2Dæ•°ç»„ï¼‰
                if audio_array.ndim == 1:
                    audio_array = audio_array.reshape(1, -1)
                
                return audio_array, sample_rate
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_file_path)
                except Exception as e:
                    logger.warning(f"æ¸…ç†éŸ³é¢‘è½¬æ¢ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            raise
    
    async def _get_audio_duration(self, audio_file_path: str) -> float:
        """è·å–éŸ³é¢‘æ–‡ä»¶æ—¶é•¿ï¼ˆç§’ï¼‰"""
        try:
            # ä½¿ç”¨audio_converterè·å–éŸ³é¢‘ä¿¡æ¯
            audio_info = await audio_converter.get_audio_info(audio_file_path)
            return audio_info.duration
        except Exception as e:
            logger.error(f"âŒ è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
            return 0.0
    
    async def _cleanup_temp_files(self, temp_audio_path: str, was_converted: bool, converted_file_path: Optional[str]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            # æ¸…ç†åŸå§‹ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_audio_path):
                os.unlink(temp_audio_path)
                logger.debug(f"ğŸ—‘ï¸ æ¸…ç†åŸå§‹ä¸´æ—¶æ–‡ä»¶: {temp_audio_path}")
            
            # æ¸…ç†è½¬æ¢åçš„æ–‡ä»¶
            if was_converted and converted_file_path:
                audio_converter.cleanup_converted_file(converted_file_path, was_converted)
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    def _merge_adjacent_short_segments(self, segments: List[SpeakerSegment]) -> List[SpeakerSegment]:
        """
        Merge adjacent short segments of the same speaker
        If adjacent segments are from the same speaker and each segment duration is less than 5s, merge them
        """
        if not segments:
            return segments
        
        merged_segments = []
        current_segment = segments[0]
        
        for segment in segments[1:]:
            # Check if current and next segment are from same speaker and both are short (< 5s)
            current_duration = current_segment.end_time - current_segment.start_time
            next_duration = segment.end_time - segment.start_time
            
            if (current_segment.speaker_label == segment.speaker_label and 
                current_duration < 5.0 and next_duration < 5.0):
                # Merge segments
                logger.debug(f"ğŸ”— åˆå¹¶ç‰‡æ®µ: {current_segment.speaker_label} "
                           f"[{current_segment.start_time:.1f}s-{current_segment.end_time:.1f}s] + "
                           f"[{segment.start_time:.1f}s-{segment.end_time:.1f}s]")
                
                current_segment = SpeakerSegment(
                    start_time=current_segment.start_time,
                    end_time=segment.end_time,
                    speaker_label=current_segment.speaker_label,
                    duration=segment.end_time - current_segment.start_time
                )
            else:
                # No merge, add current segment to result and move to next
                merged_segments.append(current_segment)
                current_segment = segment
        
        # Add the last segment
        merged_segments.append(current_segment)
        
        # Second pass: merge segments shorter than 2s to next segment (regardless of speaker)
        final_segments = []
        i = 0
        while i < len(merged_segments):
            current_seg = merged_segments[i]
            current_duration = current_seg.end_time - current_seg.start_time
            
            # If current segment is shorter than 2s and not the last segment
            if current_duration < 2.0 and i < len(merged_segments) - 1:
                next_seg = merged_segments[i + 1]
                
                logger.debug(f"ğŸ”— å¼ºåˆ¶åˆå¹¶çŸ­ç‰‡æ®µ: {current_seg.speaker_label} "
                           f"[{current_seg.start_time:.1f}s-{current_seg.end_time:.1f}s] -> "
                           f"{next_seg.speaker_label} [{next_seg.start_time:.1f}s-{next_seg.end_time:.1f}s]")
                
                # Merge current short segment to next segment
                merged_segment = SpeakerSegment(
                    start_time=current_seg.start_time,
                    end_time=next_seg.end_time,
                    speaker_label=next_seg.speaker_label,  # Keep next segment's speaker label
                    duration=next_seg.end_time - current_seg.start_time
                )
                
                final_segments.append(merged_segment)
                i += 2  # Skip next segment as it's already merged
            else:
                # If last segment is shorter than 2s, merge it to previous segment
                if (current_duration < 2.0 and i == len(merged_segments) - 1 and 
                    len(final_segments) > 0):
                    
                    prev_seg = final_segments.pop()  # Remove last segment from final_segments
                    
                    logger.debug(f"ğŸ”— åˆå¹¶æœ€åçŸ­ç‰‡æ®µ: {prev_seg.speaker_label} "
                               f"[{prev_seg.start_time:.1f}s-{prev_seg.end_time:.1f}s] + "
                               f"{current_seg.speaker_label} [{current_seg.start_time:.1f}s-{current_seg.end_time:.1f}s]")
                    
                    merged_segment = SpeakerSegment(
                        start_time=prev_seg.start_time,
                        end_time=current_seg.end_time,
                        speaker_label=prev_seg.speaker_label,  # Keep previous segment's speaker label
                        duration=current_seg.end_time - prev_seg.start_time
                    )
                    
                    final_segments.append(merged_segment)
                else:
                    final_segments.append(current_seg)
                i += 1
        
        # Third pass: remove segments shorter than 1s
        filtered_segments = []
        removed_count = 0
        
        for segment in final_segments:
            segment_duration = segment.end_time - segment.start_time
            if segment_duration >= 1.0:
                filtered_segments.append(segment)
            else:
                removed_count += 1
                logger.debug(f"ğŸ—‘ï¸ ç§»é™¤çŸ­ç‰‡æ®µ: {segment.speaker_label} "
                           f"[{segment.start_time:.1f}s-{segment.end_time:.1f}s] "
                           f"æ—¶é•¿: {segment_duration:.2f}s")
        
        original_count = len(segments)
        final_count = len(filtered_segments)
        
        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ ç§»é™¤äº† {removed_count} ä¸ªå°äº1ç§’çš„çŸ­ç‰‡æ®µ")
        
        if final_count < original_count:
            logger.info(f"âœ… ç‰‡æ®µä¼˜åŒ–å®Œæˆ: {original_count} -> {final_count} ä¸ªç‰‡æ®µ")
        
        return filtered_segments


# åˆ›å»ºå…¨å±€å®ä¾‹
audio_processing_service = AudioProcessingService() 