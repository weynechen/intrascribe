"""
è¯´è¯äººåˆ†ç¦»æœåŠ¡
åŸºäºpyannote.audioå®ç°éŸ³é¢‘çš„è¯´è¯äººè¯†åˆ«å’Œåˆ†ç¦»
"""
import logging
import os
import tempfile
from typing import List, Dict, Any, Tuple
import numpy as np
from dataclasses import dataclass
import librosa

import io

logger = logging.getLogger(__name__)

@dataclass
class SpeakerSegment:
    """è¯´è¯äººç‰‡æ®µæ•°æ®ç»“æ„"""
    start_time: float  # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time: float    # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    speaker_label: str # è¯´è¯äººæ ‡è¯†
    duration: float    # æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰


class SpeakerDiarizationService:
    """è¯´è¯äººåˆ†ç¦»æœåŠ¡"""
    
    def __init__(self):
        self.pipeline = None
        self._initialize_pipeline()
    
    def _initialize_pipeline(self):
        """åˆå§‹åŒ–pyannote pipeline"""
        try:
            from pyannote.audio import Pipeline
            import torch
            from .config import settings
            
            # Check if HuggingFace token is configured
            if not settings.huggingface_token:
                logger.error("âŒ HUGGINGFACE_TOKENæœªé…ç½®åœ¨ç¯å¢ƒå˜é‡ä¸­")
                raise Exception("HUGGINGFACE_TOKEN required for pyannote.audio")
            
            # Initialize speaker diarization pipeline
            self.pipeline = Pipeline.from_pretrained(
                settings.pyannote_model,
                use_auth_token=settings.huggingface_token,
            )
            
            # Send pipeline to GPU if available
            if torch.cuda.is_available():
                logger.info("ğŸ”¥ ä½¿ç”¨GPUè¿›è¡Œè¯´è¯äººåˆ†ç¦»")
                self.pipeline.to(torch.device("cuda"))
            else:
                logger.info("ğŸ’» ä½¿ç”¨CPUè¿›è¡Œè¯´è¯äººåˆ†ç¦»")
            
            logger.info("ğŸ¤ è¯´è¯äººåˆ†ç¦»æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except ImportError as e:
            logger.error(f"âŒ pyannote.audioæœªå®‰è£…: {e}")
            self.pipeline = None
        except Exception as e:
            logger.error(f"âŒ è¯´è¯äººåˆ†ç¦»æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.pipeline = None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.pipeline is not None
    
    async def diarize_audio_file(self, audio_file_path: str) -> List[SpeakerSegment]:
        """
        å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯´è¯äººåˆ†ç¦»
        
        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[SpeakerSegment]: è¯´è¯äººç‰‡æ®µåˆ—è¡¨
        """
        if not self.is_available():
            logger.warning("è¯´è¯äººåˆ†ç¦»æœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›å•ä¸€è¯´è¯äºº")
            return await self._fallback_single_speaker(audio_file_path)
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è¯´è¯äººåˆ†ç¦»: {audio_file_path}")
            
            # Apply pretrained pipeline
            diarization = self.pipeline(audio_file_path)
            
            segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segment = SpeakerSegment(
                    start_time=turn.start,
                    end_time=turn.end,
                    speaker_label=f"{speaker}",
                    duration=turn.end - turn.start
                )
                segments.append(segment)
                
                logger.debug(f"ğŸ—£ï¸ å‘ç°è¯´è¯äººç‰‡æ®µ: {segment.speaker_label} "
                           f"[{segment.start_time:.1f}s - {segment.end_time:.1f}s] "
                           f"æ—¶é•¿: {segment.duration:.1f}s")
            
            # Remove overlapping segments (keep longer ones)
            segments = self._remove_overlapping_segments(segments)
            
            logger.info(f"âœ… è¯´è¯äººåˆ†ç¦»å®Œæˆï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
            return segments
            
        except Exception as e:
            logger.error(f"âŒ è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
            return await self._fallback_single_speaker(audio_file_path)
    
    async def diarize_audio_data(self, audio_data: bytes, format: str = "mp3") -> List[SpeakerSegment]:
        """
        å¯¹éŸ³é¢‘æ•°æ®è¿›è¡Œè¯´è¯äººåˆ†ç¦»
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            format: éŸ³é¢‘æ ¼å¼
            
        Returns:
            List[SpeakerSegment]: è¯´è¯äººç‰‡æ®µåˆ—è¡¨
        """
        # Create temporary file
        with tempfile.NamedTemporaryFile(suffix=f".{format}", delete=False) as temp_file:
            temp_file.write(audio_data)
            temp_file_path = temp_file.name
        
        # Convert to WAV if needed
        processed_file_path = temp_file_path
        was_converted = False
        
        try:
            # Import audio converter here to avoid circular imports
            from .audio_converter import audio_converter
            
            if format.lower() in ['mp3', 'mpeg']:
                logger.info("ğŸ”„ è¯´è¯äººåˆ†ç¦»ï¼šæ£€æµ‹åˆ°MP3ï¼Œè½¬æ¢ä¸ºWAVå¤„ç†...")
                processed_file_path, was_converted = await audio_converter.convert_to_wav_if_needed(
                    temp_file_path, format
                )
            
            # Process the audio file
            segments = await self.diarize_audio_file(processed_file_path)
            return segments
            
        finally:
            # Clean up temporary files
            try:
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                
                if was_converted and processed_file_path != temp_file_path:
                    from .audio_converter import audio_converter
                    audio_converter.cleanup_converted_file(processed_file_path, was_converted)
                    
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    def _remove_overlapping_segments(self, segments: List[SpeakerSegment]) -> List[SpeakerSegment]:
        """
        ç§»é™¤é‡å çš„è¯´è¯äººç‰‡æ®µï¼Œä¿ç•™æ—¶é—´è¾ƒé•¿çš„
        
        Args:
            segments: åŸå§‹ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            List[SpeakerSegment]: å»é‡åçš„ç‰‡æ®µåˆ—è¡¨
        """
        if not segments:
            return []
        
        # Sort segments by start time
        sorted_segments = sorted(segments, key=lambda x: x.start_time)
        
        filtered_segments = []
        
        for current in sorted_segments:
            # Check for overlap with existing segments
            overlapping = False
            
            for i, existing in enumerate(filtered_segments):
                if self._segments_overlap(existing, current):
                    # Keep the longer segment
                    if current.duration > existing.duration:
                        logger.debug(f"ğŸ”„ æ›¿æ¢è¾ƒçŸ­ç‰‡æ®µ: {existing.speaker_label} "
                                   f"[{existing.start_time:.1f}s-{existing.end_time:.1f}s] "
                                   f"-> {current.speaker_label} "
                                   f"[{current.start_time:.1f}s-{current.end_time:.1f}s]")
                        filtered_segments[i] = current
                    else:
                        logger.debug(f"â­ï¸ è·³è¿‡è¾ƒçŸ­ç‰‡æ®µ: {current.speaker_label} "
                                   f"[{current.start_time:.1f}s-{current.end_time:.1f}s]")
                    overlapping = True
                    break
            
            if not overlapping:
                filtered_segments.append(current)
        
        # Sort by start time again
        filtered_segments.sort(key=lambda x: x.start_time)
        
        logger.info(f"ğŸ”„ å»é‡å®Œæˆ: {len(segments)} -> {len(filtered_segments)} ä¸ªç‰‡æ®µ")
        return filtered_segments
    
    def _segments_overlap(self, seg1: SpeakerSegment, seg2: SpeakerSegment) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªç‰‡æ®µæ˜¯å¦é‡å """
        return not (seg1.end_time <= seg2.start_time or seg2.end_time <= seg1.start_time)
    
    async def _fallback_single_speaker(self, audio_file_path: str) -> List[SpeakerSegment]:
        """
        å›é€€æ–¹æ¡ˆï¼šå°†æ•´ä¸ªéŸ³é¢‘ä½œä¸ºå•ä¸€è¯´è¯äººå¤„ç†
        
        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[SpeakerSegment]: å•ä¸€è¯´è¯äººç‰‡æ®µ
        """
        try:
            # Get audio duration using audio_converter
            from .audio_converter import audio_converter
            audio_info = await audio_converter.get_audio_info(audio_file_path)
            duration = audio_info.duration
            
            segment = SpeakerSegment(
                start_time=0.0,
                end_time=duration,
                speaker_label="speaker_0",
                duration=duration
            )
            
            logger.info(f"ğŸ“± å›é€€åˆ°å•ä¸€è¯´è¯äººæ¨¡å¼ï¼Œæ—¶é•¿: {duration:.1f}s")
            return [segment]
            
        except Exception as e:
            logger.error(f"âŒ è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
            # Final fallback - assume 60 seconds
            return [SpeakerSegment(
                start_time=0.0,
                end_time=60.0,
                speaker_label="speaker_0",
                duration=60.0
            )]
    
    async def _split_single_segment(self, audio_file_path: str, segment: SpeakerSegment) -> Tuple[SpeakerSegment, bytes]:
        """
        åˆ†å‰²å•ä¸ªéŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            segment: è¯´è¯äººç‰‡æ®µ
            
        Returns:
            Tuple[SpeakerSegment, bytes]: ç‰‡æ®µä¿¡æ¯å’Œå¯¹åº”çš„éŸ³é¢‘æ•°æ®
        """
        import tempfile
        import subprocess
        import asyncio
        from .audio_converter import audio_converter
        
        # Create unique temp file for this segment
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_output:
            temp_output_path = temp_output.name
        
        try:
            if not audio_converter.ffmpeg_path:
                raise Exception("ffmpegä¸å¯ç”¨ï¼Œæ— æ³•åˆ†å‰²éŸ³é¢‘")
            
            # Use ffmpeg to extract segment
            cmd = [
                audio_converter.ffmpeg_path,
                '-i', audio_file_path,
                '-ss', str(segment.start_time),      # Start time
                '-t', str(segment.duration),         # Duration
                '-acodec', 'pcm_s16le',             # 16-bit PCM
                '-ar', '16000',                     # 16kHz sample rate
                '-ac', '1',                         # Mono
                '-y',                               # Overwrite
                temp_output_path
            ]
            
            logger.debug(f"ğŸ”§ å¹¶è¡Œæ‰§è¡ŒffmpegéŸ³é¢‘åˆ†å‰²: {segment.speaker_label} "
                        f"[{segment.start_time:.1f}s-{segment.end_time:.1f}s]")
            
            # Run ffmpeg asynchronously
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=60)
            
            if process.returncode != 0:
                error_msg = f"ffmpegéŸ³é¢‘åˆ†å‰²å¤±è´¥: {stderr.decode()}"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)
            
            # Read the audio data
            with open(temp_output_path, 'rb') as f:
                audio_bytes = f.read()
            
            logger.debug(f"âœ‚ï¸ å®ŒæˆéŸ³é¢‘ç‰‡æ®µåˆ†å‰²: {segment.speaker_label} "
                       f"[{segment.start_time:.1f}s-{segment.end_time:.1f}s] "
                       f"å¤§å°: {len(audio_bytes)} bytes")
            
            return (segment, audio_bytes)
            
        finally:
            # Clean up temp file
            try:
                if os.path.exists(temp_output_path):
                    os.unlink(temp_output_path)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    async def split_audio_by_segments(self, audio_file_path: str, segments: List[SpeakerSegment]) -> List[Tuple[SpeakerSegment, bytes]]:
        """
        æ ¹æ®è¯´è¯äººç‰‡æ®µåˆ†å‰²éŸ³é¢‘ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
        
        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            segments: è¯´è¯äººç‰‡æ®µåˆ—è¡¨
            
        Returns:
            List[Tuple[SpeakerSegment, bytes]]: ç‰‡æ®µä¿¡æ¯å’Œå¯¹åº”çš„éŸ³é¢‘æ•°æ®
        """
        try:
            import asyncio
            
            logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡ŒéŸ³é¢‘åˆ†å‰²ï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
            
            # Create tasks for parallel processing
            tasks = [
                self._split_single_segment(audio_file_path, segment)
                for segment in segments
            ]
            
            # Execute all tasks in parallel
            split_audios = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out exceptions and log errors
            successful_splits = []
            failed_count = 0
            
            for i, result in enumerate(split_audios):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ç‰‡æ®µ {segments[i].speaker_label} åˆ†å‰²å¤±è´¥: {result}")
                    failed_count += 1
                else:
                    successful_splits.append(result)
            
            if failed_count > 0:
                logger.warning(f"âš ï¸ {failed_count} ä¸ªç‰‡æ®µåˆ†å‰²å¤±è´¥")
            
            logger.info(f"âœ… å¹¶è¡ŒéŸ³é¢‘åˆ†å‰²å®Œæˆï¼ŒæˆåŠŸ {len(successful_splits)} ä¸ªç‰‡æ®µ")
            return successful_splits
            
        except Exception as e:
            logger.error(f"âŒ å¹¶è¡ŒéŸ³é¢‘åˆ†å‰²å¤±è´¥: {e}")
            raise


# å…¨å±€å®ä¾‹
speaker_diarization_service = SpeakerDiarizationService() 