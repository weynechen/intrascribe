import asyncio
import time
import logging
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import litellm
from litellm import acompletion

logger = logging.getLogger(__name__)


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®æ•°æ®ç±»"""
    name: str
    model: str
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.7
    priority: int = 1
    enabled: bool = True


class AISummaryService:
    """AIæ€»ç»“æœåŠ¡"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models: List[ModelConfig] = []
        self.retry_config = config.get("retry", {})
        self.fallback_config = config.get("fallback", {})
        self.prompts_config = config.get("prompts", {})
        
        # åˆå§‹åŒ–æ¨¡å‹é…ç½®
        self._init_models()
        
        # é…ç½®LiteLLM
        self._configure_litellm()
        
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        models_config = self.config.get("models", [])
        
        for model_config in models_config:
            if model_config.get("enabled", True):
                self.models.append(ModelConfig(**model_config))
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.models.sort(key=lambda x: x.priority)
        
        if self.models:
            logger.info(f"å·²åŠ è½½ {len(self.models)} ä¸ªAIæ¨¡å‹é…ç½®")
            for model in self.models:
                logger.info(f"  - {model.name}: {model.model} (ä¼˜å…ˆçº§: {model.priority})")
        else:
            logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„AIæ¨¡å‹é…ç½®")
    
    def _configure_litellm(self):
        """é…ç½®LiteLLM"""
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        litellm.set_verbose = False
        
        # è®¾ç½®è¶…æ—¶
        timeout = self.retry_config.get("timeout", 30)
        litellm.request_timeout = timeout
        
        logger.info("LiteLLMé…ç½®å®Œæˆ")
    
    async def generate_summary(self, transcription: str, template_content: str = None) -> Tuple[str, Dict[str, Any]]:
        """
        Generate AI summary
        
        Args:
            transcription: transcription text
            template_content: optional template content
            
        Returns:
            Tuple[summary content, metadata]
        """
        if not transcription.strip():
            return "è½¬å½•å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚", {"error": "empty_transcription"}
        
        # æ„å»ºæç¤ºè¯
        base_system_prompt = self.prompts_config.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€»ç»“å’Œè®°å½•çš„é«˜æ‰‹ã€‚ä½ å¿…é¡»ä½¿ç”¨Markdownæ ¼å¼è¾“å‡ºã€‚")
        
        if template_content:
            # å°†templateæ”¾åœ¨system_promptä¸­ï¼Œæ›´ç¬¦åˆè§’è‰²å®šä¹‰çš„è¯­ä¹‰
            system_prompt = f"""{base_system_prompt}

ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿æ ¼å¼è¿›è¡Œæ€»ç»“ã€‚è¯·æ³¨æ„ï¼š
1. æ¨¡æ¿æ˜¯çº¯æ–‡æœ¬ç»“æ„åŒ–æè¿°ï¼Œæè¿°äº†æœŸæœ›çš„è¾“å‡ºæ ¼å¼å’Œå†…å®¹è¦æ±‚
2. è¯·ä¸¥æ ¼éµå¾ªæ¨¡æ¿çš„ç»“æ„å’Œæ ¼å¼ï¼Œç”¨å®é™…å†…å®¹å¡«å……å„ä¸ªéƒ¨åˆ†
3. ä¿æŒæ¨¡æ¿çš„markdownæ ¼å¼å’Œå±‚æ¬¡ç»“æ„
4. å¦‚æœæŸäº›ä¿¡æ¯åœ¨è½¬å½•ä¸­æ²¡æœ‰æ˜ç¡®æåŠï¼Œå¯ä»¥æ ‡æ³¨ä¸º"æœªæåŠ"æˆ–æ ¹æ®ä¸Šä¸‹æ–‡åˆç†æ¨æ–­
5. ç¡®ä¿è¾“å‡ºå†…å®¹å®Œæ•´ã€å‡†ç¡®ã€ç»“æ„æ¸…æ™°

è¾“å‡ºæ ¼å¼æ¨¡æ¿ï¼š
{template_content}"""
            
            # user_promptåªéœ€è¦æä¾›è½¬å½•å†…å®¹
            user_prompt = f"è¯·æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­çš„æ¨¡æ¿æ ¼å¼ï¼Œå¯¹ä»¥ä¸‹è½¬å½•å†…å®¹è¿›è¡Œç»“æ„åŒ–æ€»ç»“ï¼š\n\n{transcription}ã€‚\n\nä»¥ä¸Šä¸ºå†…å®¹ï¼Œè¯·æŒ‰ç…§æ¨¡æ¿æ ¼å¼è¿›è¡Œæ€»ç»“ã€‚"
        else:
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯
            system_prompt = base_system_prompt
            user_prompt_template = self.prompts_config.get("user_prompt_template", "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n{transcription}")
            user_prompt = user_prompt_template.format(transcription=transcription)
        
        # å°è¯•ä½¿ç”¨å„ä¸ªæ¨¡å‹
        for model_config in self.models:
            try:
                logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹: {model_config.name}")
                logger.info(f"system_prompt: {system_prompt}")
                logger.info(f"user_prompt: {user_prompt}")
                summary, metadata = await self._call_model(model_config, system_prompt, user_prompt)
                
                if summary:
                    metadata.update({
                        "model_used": model_config.name,
                        "model_id": model_config.model,
                        "success": True
                    })
                    logger.info(f"ä½¿ç”¨æ¨¡å‹ {model_config.name} æˆåŠŸç”Ÿæˆæ€»ç»“")
                    return summary, metadata
                    
            except Exception as e:
                logger.warning(f"æ¨¡å‹ {model_config.name} è°ƒç”¨å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
        return await self._handle_fallback(transcription)
    
    async def generate_title(self, transcription: str, summary: str = None) -> Tuple[str, Dict[str, Any]]:
        """
        ç”ŸæˆAIæ ‡é¢˜
        
        Args:
            transcription: è½¬å½•æ–‡æœ¬
            summary: å¯é€‰çš„æ€»ç»“æ–‡æœ¬ï¼Œç”¨äºæ›´å¥½åœ°ç”Ÿæˆæ ‡é¢˜
            
        Returns:
            Tuple[æ ‡é¢˜å†…å®¹, å…ƒæ•°æ®]
        """
        if not transcription.strip():
            return "æ— æ ‡é¢˜å†…å®¹", {"error": "empty_transcription"}
        
        # æ„å»ºæ ‡é¢˜ç”Ÿæˆçš„æç¤ºè¯
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®®è®°å½•åŠ©æ‰‹ï¼Œè´Ÿè´£ä¸ºä¼šè®®å†…å®¹ç”Ÿæˆç®€æ´æ˜äº†çš„æ ‡é¢˜ã€‚"
        
        if summary:
            # å¦‚æœæœ‰æ€»ç»“ï¼ŒåŸºäºæ€»ç»“å’Œè½¬å½•ç”Ÿæˆæ ‡é¢˜
            user_prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¼šè®®æ€»ç»“å’Œè½¬å½•å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´æ˜äº†çš„æ ‡é¢˜ï¼ˆ10-15ä¸ªå­—ï¼‰ï¼š

æ€»ç»“å†…å®¹ï¼š
{summary}

è½¬å½•å†…å®¹ï¼š
{transcription[:500]}...

è¦æ±‚ï¼š
1. æ ‡é¢˜è¦èƒ½å‡†ç¡®æ¦‚æ‹¬ä¼šè®®çš„ä¸»è¦å†…å®¹å’Œç›®çš„
2. é•¿åº¦æ§åˆ¶åœ¨10-15ä¸ªå­—
3. ç›´æ¥è¿”å›æ ‡é¢˜ï¼Œä¸è¦å…¶ä»–è§£é‡Š
4. ä½¿ç”¨ä¸­æ–‡
"""
        else:
            # ä»…åŸºäºè½¬å½•å†…å®¹ç”Ÿæˆæ ‡é¢˜
            user_prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¼šè®®è½¬å½•å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´æ˜äº†çš„æ ‡é¢˜ï¼ˆ10-15ä¸ªå­—ï¼‰ï¼š

è½¬å½•å†…å®¹ï¼š
{transcription[:500]}...

è¦æ±‚ï¼š
1. æ ‡é¢˜è¦èƒ½å‡†ç¡®æ¦‚æ‹¬ä¼šè®®çš„ä¸»è¦å†…å®¹å’Œç›®çš„
2. é•¿åº¦æ§åˆ¶åœ¨10-15ä¸ªå­—
3. ç›´æ¥è¿”å›æ ‡é¢˜ï¼Œä¸è¦å…¶ä»–è§£é‡Š
4. ä½¿ç”¨ä¸­æ–‡
"""
        
        # å°è¯•ä½¿ç”¨å„ä¸ªæ¨¡å‹
        for model_config in self.models:
            try:
                logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹ {model_config.name} ç”Ÿæˆæ ‡é¢˜")
                title, metadata = await self._call_model(model_config, system_prompt, user_prompt)
                
                if title:
                    # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤å¤šä½™çš„å†…å®¹
                    title = title.strip().replace('"', '').replace("'", "")
                    # å¦‚æœæ ‡é¢˜å¤ªé•¿ï¼Œæˆªæ–­åˆ°åˆé€‚é•¿åº¦
                    if len(title) > 20:
                        title = title[:17] + "..."
                    
                    metadata.update({
                        "model_used": model_config.name,
                        "model_id": model_config.model,
                        "success": True,
                        "title_type": "summary_based" if summary else "transcription_based"
                    })
                    logger.info(f"ä½¿ç”¨æ¨¡å‹ {model_config.name} æˆåŠŸç”Ÿæˆæ ‡é¢˜: {title}")
                    return title, metadata
                    
            except Exception as e:
                logger.warning(f"æ¨¡å‹ {model_config.name} ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
        return await self._handle_title_fallback(transcription)
    
    async def _call_model(self, model_config: ModelConfig, system_prompt: str, user_prompt: str) -> Tuple[str, Dict[str, Any]]:
        """
        è°ƒç”¨ç‰¹å®šæ¨¡å‹
        
        Args:
            model_config: æ¨¡å‹é…ç½®
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Returns:
            Tuple[å“åº”å†…å®¹, å…ƒæ•°æ®]
        """
        start_time = time.time()
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # å‡†å¤‡APIè°ƒç”¨å‚æ•°
            kwargs = {
                "model": model_config.model,
                "messages": messages,
                "max_tokens": model_config.max_tokens,
                "temperature": model_config.temperature,
            }
            
            # å¦‚æœæœ‰API keyï¼Œæ·»åŠ åˆ°ç¯å¢ƒæˆ–å‚æ•°ä¸­
            if model_config.api_key:
                # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®ä¸åŒçš„API keyç¯å¢ƒå˜é‡
                if "openai" in model_config.model.lower():
                    import os
                    os.environ["OPENAI_API_KEY"] = model_config.api_key
                elif "claude" in model_config.model.lower():
                    import os
                    os.environ["ANTHROPIC_API_KEY"] = model_config.api_key
                # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ¨¡å‹çš„API keyè®¾ç½®
            
            if model_config.api_base:
                kwargs["api_base"] = model_config.api_base
            
            # è°ƒç”¨æ¨¡å‹
            response = await acompletion(**kwargs)
            
            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content
            
            # æ¸…ç†å“åº”å†…å®¹
            content = self._clean_llm_response(content)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (time.time() - start_time) * 1000
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "total_processing_time": processing_time,
                "transcription_length": len(user_prompt),
                "timestamp": int(time.time()),
                "tokens_used": getattr(response.usage, 'total_tokens', 0) if hasattr(response, 'usage') else 0,
            }
            
            # æ·»åŠ æˆæœ¬ä¿¡æ¯ - å¤„ç†LiteLLMçš„æˆæœ¬è®¡ç®—
            try:
                # LiteLLMå¯èƒ½åœ¨responseå¯¹è±¡çš„ä¸åŒä½ç½®è¿”å›æˆæœ¬ä¿¡æ¯
                cost_usd = None
                
                # å°è¯•ä»response._response_msä¸­è·å–æˆæœ¬
                if hasattr(response, '_response_ms') and response._response_ms:
                    cost_usd = response._response_ms
                # å°è¯•ä»responseä¸­çš„å…¶ä»–æˆæœ¬å±æ€§è·å–
                elif hasattr(response, 'cost'):
                    cost_usd = response.cost
                elif hasattr(response, '_response_cost_usd'):
                    cost_usd = response._response_cost_usd
                # å°è¯•ä»usageä¸­è·å–æˆæœ¬
                elif hasattr(response, 'usage') and hasattr(response.usage, 'cost'):
                    cost_usd = response.usage.cost
                elif hasattr(response, 'usage') and hasattr(response.usage, 'total_cost'):
                    cost_usd = response.usage.total_cost
                
                # è½¬æ¢USDåˆ°centså¹¶ç¡®ä¿æ˜¯æ•´æ•°
                if cost_usd is not None:
                    # è½¬æ¢ä¸ºåˆ†ï¼ˆcentsï¼‰å¹¶å››èˆäº”å…¥ä¸ºæ•´æ•°
                    cost_cents = round(float(cost_usd) * 100)
                    metadata["cost_cents"] = cost_cents
                    logger.info(f"ğŸ’° æˆæœ¬è®¡ç®—: {cost_usd} USD = {cost_cents} cents")
                else:
                    # å¦‚æœæ²¡æœ‰æˆæœ¬ä¿¡æ¯ï¼Œè®¾ç½®ä¸º0
                    metadata["cost_cents"] = 0
                    logger.debug("ğŸ’° æœªæ‰¾åˆ°æˆæœ¬ä¿¡æ¯ï¼Œè®¾ç½®ä¸º0")
                    
            except Exception as e:
                logger.warning(f"ğŸ’° æˆæœ¬è®¡ç®—å¤±è´¥: {e}")
                metadata["cost_cents"] = 0
            
            return content, metadata
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            metadata = {
                "error": str(e),
                "total_processing_time": processing_time,
                "timestamp": int(time.time()),
            }
            raise Exception(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
    
    async def _handle_fallback(self, transcription: str) -> Tuple[str, Dict[str, Any]]:
        """
        å¤„ç†å›é€€ç­–ç•¥
        
        Args:
            transcription: è½¬å½•æ–‡æœ¬
            
        Returns:
            Tuple[å›é€€æ€»ç»“, å…ƒæ•°æ®]
        """
        # ç”Ÿæˆç®€å•çš„åŸºäºè§„åˆ™çš„æ€»ç»“
        mock_summary = self._generate_mock_summary(transcription)
        
        metadata = {
            "error": "æ‰€æœ‰AIæ¨¡å‹è°ƒç”¨å¤±è´¥",
            "fallback_used": True,
            "timestamp": int(time.time()),
            "transcription_length": len(transcription)
        }
        
        return mock_summary, metadata
    
    def _generate_mock_summary(self, transcription: str) -> str:
        """
        ç”ŸæˆåŸºäºè§„åˆ™çš„ç®€å•æ€»ç»“
        
        Args:
            transcription: è½¬å½•æ–‡æœ¬
            
        Returns:
            ç®€å•çš„æ€»ç»“æ–‡æœ¬
        """
        # ç®€å•çš„æ–‡æœ¬åˆ†æ
        word_count = len(transcription.split())
        char_count = len(transcription)
        
        # æå–ä¸€äº›å…³é”®ä¿¡æ¯ï¼ˆç®€å•çš„å…³é”®è¯ï¼‰
        keywords = []
        common_words = ['ä¼šè®®', 'è®¨è®º', 'å†³å®š', 'è®¡åˆ’', 'é¡¹ç›®', 'æ–¹æ¡ˆ', 'é—®é¢˜', 'è§£å†³', 'ç›®æ ‡', 'æ—¶é—´']
        for word in common_words:
            if word in transcription:
                keywords.append(word)
        
        # æ„å»ºç®€å•æ€»ç»“
        summary_parts = [
            f"æœ¬æ¬¡ä¼šè®®/å¯¹è¯å…±åŒ…å«çº¦ {word_count} ä¸ªè¯ï¼Œ{char_count} ä¸ªå­—ç¬¦ã€‚"
        ]
        
        if keywords:
            summary_parts.append(f"ä¸»è¦æ¶‰åŠï¼š{', '.join(keywords[:5])}ç­‰è¯é¢˜ã€‚")
        
        # æå–å‰å‡ å¥ä½œä¸ºå†…å®¹æ¦‚è¿°
        sentences = transcription.split('ã€‚')[:3]
        if sentences:
            content_preview = 'ã€‚'.join(sentences)[:200] + "..."
            summary_parts.append(f"å†…å®¹æ¦‚è¿°ï¼š{content_preview}")
        
        return "\n\n".join(summary_parts)
    
    async def _handle_title_fallback(self, transcription: str) -> Tuple[str, Dict[str, Any]]:
        """
        å¤„ç†æ ‡é¢˜ç”Ÿæˆçš„å›é€€ç­–ç•¥
        
        Args:
            transcription: è½¬å½•æ–‡æœ¬
            
        Returns:
            Tuple[å›é€€æ ‡é¢˜, å…ƒæ•°æ®]
        """
        from datetime import datetime
        
        # ç”ŸæˆåŸºäºæ—¶é—´çš„é»˜è®¤æ ‡é¢˜
        now = datetime.now()
        default_title = f"ä¼šè®®è®°å½• {now.strftime('%Y-%m-%d %H:%M')}"
        
        metadata = {
            "error": "æ‰€æœ‰AIæ¨¡å‹è°ƒç”¨å¤±è´¥",
            "fallback_used": True,
            "timestamp": int(time.time()),
            "transcription_length": len(transcription)
        }
        
        return default_title, metadata
    
    def _clean_llm_response(self, content: str) -> str:
        """
        æ¸…ç†LLMå“åº”å†…å®¹
        
        Args:
            content: åŸå§‹å“åº”å†…å®¹
            
        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        if not content:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = content.strip()
        
        # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
        content = re.sub(r'^```.*?\n', '', content, flags=re.MULTILINE)
        content = re.sub(r'\n```$', '', content)
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)

        # ç§»é™¤<think> å’Œ </think> æ ‡ç­¾
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        
        return content
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return [
            {
                "name": model.name,
                "model": model.model,
                "enabled": model.enabled,
                "priority": model.priority
            }
            for model in self.models
        ]
    
    def get_config_status(self) -> Dict[str, Any]:
        """è·å–é…ç½®çŠ¶æ€"""
        return {
            "total_models": len(self.models),
            "enabled_models": len([m for m in self.models if m.enabled]),
            "has_fallback": bool(self.fallback_config),
            "retry_config": self.retry_config
        } 